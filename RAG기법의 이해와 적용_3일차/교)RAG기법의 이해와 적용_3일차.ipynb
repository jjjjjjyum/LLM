{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸŒ¼ RAGê¸°ë²•ì˜ ì´í•´ì™€ ì ìš© - 3ì°¨ì‹œ(24.12.02)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "# ë­ì²´ì¸ì˜ ë©”ëª¨ë¦¬ í´ë˜ìŠ¤ BufferMemory\n",
    "# input ì§ˆë¬¸ìì˜ ì‘ë‹µ\n",
    "# output ì±—ë´‡ì˜ ì‘ë‹µ\n",
    "# ì„ ì… ì„ ì¶œ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coversation_chain(memory, query):\n",
    "    # ëŒ€í™” ê¸°ë¡ ì¶œë ¥\n",
    "    chat_history_data = memory.load_memory_variables()\n",
    "    print(\"Chat history data:\", chat_history_data)\n",
    "    \n",
    "    chat_history = RunnablePassthrough.assign(\n",
    "        chat_history=RunnableLambda(chat_history_data) | itemgetter(memory.memory_key)\n",
    "    )\n",
    "    \n",
    "    llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessage(content='ë„ˆëŠ” ì§„ì² í•œ AI ë¹„ì„œì•¼'),\n",
    "            MessagesPlaceholder(variable_name='chat_history'),\n",
    "            HumanMessage(content='{input}')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    chain = chat_history | prompt | llm | StrOutputParser()\n",
    "\n",
    "    # ì²´ì¸ êµ¬ì¡° ì¶œë ¥\n",
    "    print(\"Chain structure:\", chain)\n",
    "\n",
    "    try:\n",
    "        answer = chain.invoke({'input': query})\n",
    "        print(\"AI Response:\", answer)\n",
    "    except TypeError as e:\n",
    "        print(\"Error details:\", e)\n",
    "        raise  # ì˜¤ë¥˜ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œì„œ í™•ì¸\n",
    "    \n",
    "    memory.save_context(inputs={'human': query}, outputs={'ai': answer})\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    #ëª…ì‹œì ìœ¼ë¡œ inpeu output ë„£ì–´ì¤„ ìˆ˜ ìˆìŒ\n",
    "    inputs = {\n",
    "        'human':'ì•ˆë…•í•˜ì„¸ìš”. íœ´ëŒ€í°ì„ êµ¬ë§¤í•˜ëŸ¬ ì™”ì–´ìš”'\n",
    "    },\n",
    "    outputs={\n",
    "        'ai':'ì•ˆë…•í•˜ì„¸ìš”. ë¬´ì—‡ì„ ì°¾ìœ¼ì„¸ìš”?'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: ì•ˆë…•í•˜ì„¸ìš”. íœ´ëŒ€í°ì„ êµ¬ë§¤í•˜ëŸ¬ ì™”ì–´ìš”\\nAI: ì•ˆë…•í•˜ì„¸ìš”. ë¬´ì—‡ì„ ì°¾ìœ¼ì„¸ìš”?'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "[í”„ë¡œì íŠ¸ëª…]\n",
      "LangChainProject\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "logging.langsmith(\"LangChainProject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model = 'gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    # ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "    [\n",
    "        ('system','ë„ˆëŠ” ì¹œì ˆí•œ ì±—ë´‡ì´ì•¼'),\n",
    "        MessagesPlaceholder(variable_name='chat_history'),\n",
    "        # ì´ì „ ëŒ€í™”ë¥¼ ë‚´ìš©ì„ ë‹´ì„ ìë¦¬ë¥¼ ë§Œë“¤ì–´ ì¤Œ\n",
    "        ('human', '{input}')\n",
    "        # ë™ì ìœ¼ë¡œ ë‹´ì„ ìˆ˜ ìˆë„ë¡ inputìœ¼ë¡œ\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëŒ€í™” ë©”ëª¨ë¦¬ ìƒì„±\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key='chat_history')\n",
    "# ë©”ì„¸ì§€ í˜•íƒœë¡œ ë°˜í™˜í•˜ê² ë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': []}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = RunnablePassthrough.assign(\n",
    "    chat_history = RunnableLambda(memory.load_memory_variables) | itemgetter('chat_history')\n",
    "    # ë©”ëª¨ë¦¬ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  chat_history ë§Œ ì¶”ì¶œí•˜ê² ë‹¤\n",
    "    # input ë‚´ìš© ì¤‘ì—ì„œë„ chat_historyê°’ë§Œ ê°€ì ¸ì˜¤ê¸° => ëŒ€í™”ì´ë ¥ (ê¹œë”í•œ ëŒ€í™” ì´ë ¥ë§Œ ê°€ì ¸ì˜¤ë¯€ë¡œ ì „ì²˜ë¦¬ ê³¼ì •ë¼ í•  ìˆ˜ ìˆìŒ)\n",
    "    # runnable ì…ë ¥ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•œë‹¤\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = runnable | prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•ˆë…•í•˜ì„¸ìš”, ì•„ë¬´ê°œë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\n"
     ]
    }
   ],
   "source": [
    "respone = chain.invoke({'input' :'ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ ì•„ë¬´ê°œ ì…ë‹ˆë‹¤.'})\n",
    "print(respone.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({'human' : 'ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ ì•„ë¬´ê°œ ì…ë‹ˆë‹¤.'}, {'ai' : respone.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‹¹ì‹ ì˜ ì´ë¦„ì€ \"ì•„ë¬´ê°œ\"ë¼ê³  í•˜ì…¨ìŠµë‹ˆë‹¤. ë§ë‚˜ìš”?\n"
     ]
    }
   ],
   "source": [
    "respone = chain.invoke({'input' :'ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì´ ë­ë¼ê³  í–ˆì£ ?'})\n",
    "print(respone.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1, return_messages=True, memory_key='chat_history')\n",
    "# ë©”ëª¨ë¦¬ ì´ˆê¸°í™”\n",
    "# WindowMemory : ìµœê·¼ ëŒ€í™”ë§Œ ì €ì¥\n",
    "# BufferMemory : ìˆœì°¨ì ìœ¼ë¡œ ëª¨ë‘ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_chain(memory, query):\n",
    "    chat_history = RunnablePassthrough.assign(\n",
    "        chat_history = RunnableLambda(memory.load_memory_variables) | itemgetter(memory.memory_key)\n",
    "    )\n",
    "    llm = ChatOpenAI(model = 'gpt-4o', temperature = 0)\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            ('system', 'ë„ˆëŠ” ì¹œì ˆí•œ AI ë¹„ì„œì•¼'),\n",
    "            MessagesPlaceholder(variable_name='chat_history'),\n",
    "            ('human' , \"{input}\")\n",
    "        ]\n",
    "    )\n",
    "    chain = chat_history | prompt | llm | StrOutputParser()\n",
    "    # StrOutputParser() : ì¶œë ¥ë˜ëŠ” ê°’ì„ ìë™ìœ¼ë¡œ íŒŒì‹±(ë¶ˆí•„ìš”í•œ ê°œí–‰ë¬¸ì, íŠ¹ìˆ˜ê¸°í˜¸ ë“±ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬)\n",
    "    answer = chain.invoke({'input' : query})\n",
    "    memory.save_context(inputs = {'human' : query}, outputs={'ai' : answer})\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "al :  ì¶”ìš´ ë‚ ì”¨ì—ëŠ” ë”°ëœ»í•˜ê³  í¬ê·¼í•œ ìŒì‹ì„ ë¨¹ìœ¼ë©´ ëª¸ë„ ë§ˆìŒë„ ë”°ëœ»í•´ì§€ì£ . ëª‡ ê°€ì§€ ì¶”ì²œí•´ë“œë¦´ê²Œìš”:\n",
      "\n",
      "1. **ê¹€ì¹˜ì°Œê°œ**: ë§¤ì½¤í•˜ê³  ëœ¨ê±°ìš´ êµ­ë¬¼ì´ ëª¸ì„ ë”°ëœ»í•˜ê²Œ í•´ì¤ë‹ˆë‹¤. ë¼ì§€ê³ ê¸°ë‚˜ ì°¸ì¹˜ ë“±ì„ ë„£ì–´ ë‹¤ì–‘í•˜ê²Œ ì¦ê¸¸ ìˆ˜ ìˆì–´ìš”.\n",
      "\n",
      "2. **ëœì¥ì°Œê°œ**: êµ¬ìˆ˜í•œ ë§›ì´ ì¼í’ˆì¸ ëœì¥ì°Œê°œëŠ” ë‘ë¶€, ê°ì, í˜¸ë°• ë“±ì„ ë„£ì–´ í‘¸ì§í•˜ê²Œ ë“ì´ë©´ ì¢‹ìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. **ì‚¼ê³„íƒ•**: ì˜ì–‘ê°€ê°€ í’ë¶€í•œ ì‚¼ê³„íƒ•ì€ ëª¸ì„ ë”°ëœ»í•˜ê²Œ í•´ì£¼ê³  ê¸°ìš´ì„ ë¶ë‹ì•„ ì¤ë‹ˆë‹¤.\n",
      "\n",
      "4. **ì¹¼êµ­ìˆ˜**: ëœ¨ê±°ìš´ êµ­ë¬¼ì— ì«„ê¹ƒí•œ ë©´ë°œì´ ì–´ìš°ëŸ¬ì ¸ ì¶”ìš´ ë‚ ì”¨ì— ì œê²©ì…ë‹ˆë‹¤.\n",
      "\n",
      "5. **í˜¸ë–¡**: ë‹¬ì½¤í•œ ì‹œëŸ½ì´ ë“¤ì–´ê°„ í˜¸ë–¡ì€ ê°„ì‹ìœ¼ë¡œ ë¨¹ê¸° ì¢‹ê³ , ì†ì„ ë”°ëœ»í•˜ê²Œ í•´ì¤ë‹ˆë‹¤.\n",
      "\n",
      "6. **ë¶•ì–´ë¹µ**: ê¸¸ê±°ë¦¬ì—ì„œ ì‰½ê²Œ ì°¾ì„ ìˆ˜ ìˆëŠ” ë¶•ì–´ë¹µì€ ì¶”ìš´ ë‚ ì”¨ì— ë”°ëœ»í•œ ê°„ì‹ìœ¼ë¡œ ì œê²©ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì´ ì™¸ì—ë„ ë”°ëœ»í•œ ì°¨ë‚˜ ì»¤í”¼ì™€ í•¨ê»˜ ì¦ê¸°ë©´ ë”ìš± ì¢‹ê² ì£ . ë”°ëœ»í•˜ê²Œ ì…ê³  ê±´ê°• ì¡°ì‹¬í•˜ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "response = conversation_chain(\n",
    "    memory = memory,\n",
    "    query = 'ê°‘ìê¸° ë‚ ì”¨ê°€ ë„ˆë¬´ ì¶”ì›Œì¡Œì–´. ì´ ë•Œ ë¨¹ì„ë§Œí•œ ìŒì‹ì„ ì¶”ì²œí•´ì¤˜'\n",
    ")\n",
    "print('al : ', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "al :  ì¶”ìš´ ë‚ ì”¨ì— ë“£ê¸° ì¢‹ì€ ë…¸ë˜ë“¤ì€ ë”°ëœ»í•œ ë¶„ìœ„ê¸°ë¥¼ ë”í•´ì£¼ê±°ë‚˜ ê°ì„±ì„ ìê·¹í•˜ëŠ” ê³¡ë“¤ì´ ë§ìŠµë‹ˆë‹¤. ëª‡ ê°€ì§€ ì¶”ì²œí•´ë“œë¦´ê²Œìš”:\n",
      "\n",
      "1. **ì•„ì´ìœ  - \"ë°¤í¸ì§€\"**: ì”ì”í•˜ê³  ë”°ëœ»í•œ ë©œë¡œë””ê°€ ë§ˆìŒì„ í¸ì•ˆí•˜ê²Œ í•´ì¤ë‹ˆë‹¤.\n",
      "\n",
      "2. **í´í‚´ - \"ëª¨ë“  ë‚ , ëª¨ë“  ìˆœê°„\"**: ê°ë¯¸ë¡œìš´ ëª©ì†Œë¦¬ì™€ ë©œë¡œë””ê°€ ì¶”ìš´ ë‚ ì”¨ì— ì˜ ì–´ìš¸ë¦½ë‹ˆë‹¤.\n",
      "\n",
      "3. **ê¹€ê´‘ì„ - \"ì–´ëŠ 60ëŒ€ ë…¸ë¶€ë¶€ ì´ì•¼ê¸°\"**: ê°ì„±ì ì¸ ê°€ì‚¬ì™€ ë©œë¡œë””ê°€ ë§ˆìŒì„ ë”°ëœ»í•˜ê²Œ í•´ì¤ë‹ˆë‹¤.\n",
      "\n",
      "4. **ì—í”½í•˜ì´ - \"ì¶¥ë‹¤ (feat. ì´í•˜ì´)\"**: ì œëª©ì²˜ëŸ¼ ì¶”ìš´ ë‚ ì”¨ì— ë“£ê¸° ì¢‹ì€ ê³¡ì…ë‹ˆë‹¤.\n",
      "\n",
      "5. **ë°•íš¨ì‹  - \"ëˆˆì˜ ê½ƒ\"**: ê²¨ìš¸ê³¼ ì˜ ì–´ìš¸ë¦¬ëŠ” ë°œë¼ë“œë¡œ, ê°ì„±ì„ ìê·¹í•©ë‹ˆë‹¤.\n",
      "\n",
      "6. **Carpenters - \"Top of the World\"**: ë°ê³  ê²½ì¾Œí•œ ë©œë¡œë””ê°€ ê¸°ë¶„ì„ ì¢‹ê²Œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.\n",
      "\n",
      "7. **Norah Jones - \"Come Away With Me\"**: ë¶€ë“œëŸ¬ìš´ ì¬ì¦ˆí’ì˜ ê³¡ìœ¼ë¡œ, ë”°ëœ»í•œ ì»¤í”¼ í•œ ì”ê³¼ í•¨ê»˜ ë“£ê¸° ì¢‹ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì´ ë…¸ë˜ë“¤ì´ ì¶”ìš´ ë‚ ì”¨ì— ì¡°ê¸ˆì´ë‚˜ë§ˆ ë”°ëœ»í•¨ì„ ë”í•´ì£¼ê¸¸ ë°”ëë‹ˆë‹¤. ì¦ê²ê²Œ ê°ìƒí•˜ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "response = conversation_chain(\n",
    "    memory = memory,\n",
    "    query = 'ê·¸ëŸ¼ì´ëŸ´ ë•Œ ë“¤ì„ë§Œí•œ ë…¸ë˜ë¥¼ ì¶”ì²œí•´ì¤˜'\n",
    ")\n",
    "print('al : ', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "al :  ì•„ì§ ìŒì‹ì— ëŒ€í•œ ì¶”ì²œì„ ë“œë¦° ì ì´ ì—†ëŠ”ë°ìš”. í˜¹ì‹œ íŠ¹ì •í•œ ì¢…ë¥˜ì˜ ìŒì‹ì„ ì›í•˜ì‹ ë‹¤ë©´ ë§ì”€í•´ ì£¼ì‹œë©´ ì¶”ì²œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ì˜ˆë¥¼ ë“¤ì–´, í•œì‹, ì–‘ì‹, ë””ì €íŠ¸ ë“± ì–´ë–¤ ì¢…ë¥˜ì˜ ìŒì‹ì„ ì°¾ê³  ê³„ì‹ ê°€ìš”?\n"
     ]
    }
   ],
   "source": [
    "response = conversation_chain(\n",
    "    memory = memory,\n",
    "    query = 'ì•„ê¹Œ ì¶”ì²œí•´ì¤€ ìŒì‹ì¤‘ì— í•˜ë‚˜ë§Œ ì„ íƒí•´ì¤˜'\n",
    ")\n",
    "print('al : ', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='ì•„ê¹Œ ì¶”ì²œí•´ì¤€ ìŒì‹ì¤‘ì— í•˜ë‚˜ë§Œ ì„ íƒí•´ì¤˜', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ì•„ì§ ìŒì‹ì— ëŒ€í•œ ì¶”ì²œì„ ë“œë¦° ì ì´ ì—†ëŠ”ë°ìš”. í˜¹ì‹œ íŠ¹ì •í•œ ì¢…ë¥˜ì˜ ìŒì‹ì„ ì›í•˜ì‹ ë‹¤ë©´ ë§ì”€í•´ ì£¼ì‹œë©´ ì¶”ì²œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ì˜ˆë¥¼ ë“¤ì–´, í•œì‹, ì–‘ì‹, ë””ì €íŠ¸ ë“± ì–´ë–¤ ì¢…ë¥˜ì˜ ìŒì‹ì„ ì°¾ê³  ê³„ì‹ ê°€ìš”?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})['chat_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model = 'gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm = llm,\n",
    "    max_token_limit = 200,\n",
    "    return_messages =True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    inputs={\"human\": \"ì´ ì‹ë‹¹ì—ì„œ ê°€ì¥ ì¸ê¸° ìˆëŠ” ë©”ë‰´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"},\n",
    "    outputs={\n",
    "        \"ai\": \"ì´ ì‹ë‹¹ì—ì„œ ê°€ì¥ ì¸ê¸° ìˆëŠ” ë©”ë‰´ëŠ” ë¶ˆê³ ê¸° ì •ì‹, í•´ë¬¼íŒŒì „, ë¹„ë¹”ë°¥, ê·¸ë¦¬ê³  ê°ìíƒ•ì…ë‹ˆë‹¤. íŠ¹íˆ ë¶ˆê³ ê¸°ëŠ” ë‹¬ì½¤í•˜ê³  ì§­ì§¤í•œ ë§›ìœ¼ë¡œ ì™¸êµ­ì¸ ì†ë‹˜ë“¤ì—ê²Œë„ í° ì¸ê¸°ë¥¼ ëŒê³  ìˆìŠµë‹ˆë‹¤.\"\n",
    "    },\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"ì±„ì‹ì£¼ì˜ìë¥¼ ìœ„í•œ ë©”ë‰´ê°€ ì œê³µë˜ë‚˜ìš”?\"},\n",
    "    outputs={\n",
    "        \"ai\": \"ë„¤, ì±„ì‹ì£¼ì˜ìë¥¼ ìœ„í•œ ë©”ë‰´ë¡œ ì±„ì†Œ ë¹„ë¹”ë°¥, ë‘ë¶€êµ¬ì´, ì•¼ì±„ì „, ê·¸ë¦¬ê³  ë‚˜ë¬¼ ë°˜ì°¬ ì„¸íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì‹ ì„ í•œ ì œì²  ì±„ì†Œë¡œ ë§Œë“¤ì–´ì ¸ ê±´ê°•í•˜ê³  ë§›ìˆëŠ” ì‹ì‚¬ë¥¼ ì¦ê¸°ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
    "    },\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"ì–´ë¦°ì´ë¥¼ ìœ„í•œ ë©”ë‰´ë„ ìˆë‚˜ìš”?\"},\n",
    "    outputs={\n",
    "        \"ai\": \"ë„¤, ì–´ë¦°ì´ë¥¼ ìœ„í•œ ë©”ë‰´ë¡œ ë¯¸ë‹ˆ ê¹€ë°¥, ë–¡ë³¶ì´, ê·¸ë¦¬ê³  ë‹¬ì½¤í•œ ê°„ì¥ ì¹˜í‚¨ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì•„ì´ë“¤ì´ ì¢‹ì•„í•  ë§Œí•œ ë§›ê³¼ ê±´ê°•ì„ ê³ ë ¤í•œ ìš”ë¦¬ë“¤ì…ë‹ˆë‹¤.\"\n",
    "    },\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"ì´ ì‹ë‹¹ì€ ì–´ë–¤ ë¶„ìœ„ê¸°ë¥¼ ê°€ì§€ê³  ìˆë‚˜ìš”?\"},\n",
    "    outputs={\n",
    "        \"ai\": \"ì´ ì‹ë‹¹ì€ í•œì˜¥ ìŠ¤íƒ€ì¼ì˜ ì¸í…Œë¦¬ì–´ë¡œ ì „í†µì ì¸ í•œêµ­ì˜ ë¶„ìœ„ê¸°ë¥¼ ëŠë‚„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ëœ»í•˜ê³  í¸ì•ˆí•œ ì¡°ëª…ê³¼ í˜„ëŒ€ì ì¸ ë””ìì¸ì´ ì¡°í™”ë¥¼ ì´ë£¨ì–´ ê°€ì¡± ë‹¨ìœ„ ì†ë‹˜ë¿ë§Œ ì•„ë‹ˆë¼ ì—°ì¸ë“¤ì˜ ë°ì´íŠ¸ ì¥ì†Œë¡œë„ ì¸ê¸°ê°€ ë§ìŠµë‹ˆë‹¤.\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='The human asks about the most popular menu items at the restaurant. The AI responds that the most popular items are bulgogi, seafood pancake, bibimbap, and potato soup, noting that bulgogi is particularly popular among foreign guests due to its sweet and savory flavor. The human then inquires if there are menu options for vegetarians, and the AI confirms that there are vegetarian options available, including vegetable bibimbap, grilled tofu, vegetable pancakes, and a set of seasonal vegetable side dishes, all made with fresh ingredients for a healthy and delicious meal.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='ì–´ë¦°ì´ë¥¼ ìœ„í•œ ë©”ë‰´ë„ ìˆë‚˜ìš”?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ë„¤, ì–´ë¦°ì´ë¥¼ ìœ„í•œ ë©”ë‰´ë¡œ ë¯¸ë‹ˆ ê¹€ë°¥, ë–¡ë³¶ì´, ê·¸ë¦¬ê³  ë‹¬ì½¤í•œ ê°„ì¥ ì¹˜í‚¨ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì•„ì´ë“¤ì´ ì¢‹ì•„í•  ë§Œí•œ ë§›ê³¼ ê±´ê°•ì„ ê³ ë ¤í•œ ìš”ë¦¬ë“¤ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='ì´ ì‹ë‹¹ì€ ì–´ë–¤ ë¶„ìœ„ê¸°ë¥¼ ê°€ì§€ê³  ìˆë‚˜ìš”?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ì´ ì‹ë‹¹ì€ í•œì˜¥ ìŠ¤íƒ€ì¼ì˜ ì¸í…Œë¦¬ì–´ë¡œ ì „í†µì ì¸ í•œêµ­ì˜ ë¶„ìœ„ê¸°ë¥¼ ëŠë‚„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ëœ»í•˜ê³  í¸ì•ˆí•œ ì¡°ëª…ê³¼ í˜„ëŒ€ì ì¸ ë””ìì¸ì´ ì¡°í™”ë¥¼ ì´ë£¨ì–´ ê°€ì¡± ë‹¨ìœ„ ì†ë‹˜ë¿ë§Œ ì•„ë‹ˆë¼ ì—°ì¸ë“¤ì˜ ë°ì´íŠ¸ ì¥ì†Œë¡œë„ ì¸ê¸°ê°€ ë§ìŠµë‹ˆë‹¤.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The human asks about the most popular menu items at the restaurant. The AI responds that the most popular items are bulgogi, seafood pancake, bibimbap, and potato soup, noting that bulgogi is particularly popular among foreign guests due to its sweet and savory flavor. The human then inquires if there are menu options for vegetarians, and the AI confirms that there are vegetarian options available, including vegetable bibimbap, grilled tofu, vegetable pancakes, and a set of seasonal vegetable side dishes, all made with fresh ingredients for a healthy and delicious meal.'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})['history'][0].content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
