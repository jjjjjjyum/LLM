import streamlit as st
from langchain.document_loaders import WebBaseLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI
import os
from dotenv import load_dotenv
import bs4
from langchain_teddynote import logging

load_dotenv()

llm = ChatOpenAI(
    model = 'gpt-4o',
    temperature = .2,
    openai_api_key = os.getenv('OPENAI_API_KEY')
)

st.title('ë‰´ìŠ¤ ê¸°ë°˜ ëŒ€í™”í˜• ì±—ë´‡ ğŸ’¬')
st.markdown('ë‰´ìŠ¤ URLì„ ì…ë ¥í•˜ë©´ í•´ë‹¹ ë‰´ìŠ¤ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.')

# ìƒíƒœ ê´€ë¦¬ ì´ˆê¸°í™”
if 'vector_store' not in st.session_state:
    st.session_state.vector_store = None
if 'memory' not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)
if 'messages_displayed' not in st.session_state:
    st.session_state.messages_displayed = []

# ë‰´ìŠ¤ ë¡œë“œ
news_url = st.text_input('ë‰´ìŠ¤ URL ì…ë ¥ : ')
if st.button('ë‰´ìŠ¤ ë¡œë“œ'):
    if not news_url:
        st.error('URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.')
    else:
        try:
            loader = WebBaseLoader(
                web_paths = (news_url,),
                # íŠœí”Œí˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ì•¼ í•¨
                bs_kwargs = dict(
                    parse_only = bs4.SoupStrainer(
                        'div',
                        attrs={
                            'class': ['newsct_article _article_body','media_end_head_title']
                        }
                    )
                )
            )
            docs = loader.load()

            if not docs:
                st.error('ë‰´ìŠ¤ ë‚´ìš©ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. URLì„ í™•ì¸í•´ì£¼ì„¸ìš”.')
            else: 
                st.success(f'ë¬¸ì„œë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤. ë¬¸ì„œ ê°œìˆ˜ {len(docs)}')

                # ë¬¸ì„œ ë¶„í• 
                splitter = CharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)
                split_texts = splitter.split_documents(docs)

                # ì„ë² ë”©
                embeddings = OpenAIEmbeddings()
                # FAISS ë²¡í„° ì €ì¥ì†Œ ì €ì¥
                vector_store = FAISS.from_documents(split_texts, embeddings)

                st.session_state.vector_store = vector_store
        except Exception as e:
            st.error(f'ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ : {str(e)}')

prompt = st.chat_input('ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”.')
if prompt:
    if st.session_state.vector_store is None:
        st.error('ë‰´ìŠ¤ë¥¼ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.')
    else:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ê¸°ë¡
        st.session_state.memory.chat_memory.add_user_message(prompt)
        # st.session_state.messages_displayed.append({'role':'user','content':prompt})
        try:
            retriever = st.session_state.vector_store.as_retriever()
            chain = ConversationalRetrievalChain.from_llm(
                llm = llm,
                retriever = retriever,
                memory = st.session_state.memory
            )
            # aiì‘ë‹µìƒì„±
            response = chain({'question':prompt})
            ai_response = response['answer']

            # ai ë©”ì‹œì§€ ê¸°ë¡
            st.session_state.memory.chat_memory.add_ai_message(ai_response)

            st.session_state.messages_displayed.append({'role':'user','content':prompt})
            st.session_state.messages_displayed.append({'role':'assistant','content':ai_response})
        except Exception as e:
            st.error(f'ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ : {str(e)}')

# ì´ì „ ëŒ€í™” í‘œì‹œ
for message in st.session_state.messages_displayed:
    with st.chat_message(message['role']):
        st.write(message['content'])